---
title: 爬虫学习笔记
tags: 
  - 爬虫
  - 学习笔记
categories:
  - 常用工具
abbrlink: 5
---

#  基本介绍

爬虫核心：

- 获取整个网页的信息
- 解析网页中的数据
- 克服反爬虫

# urllib库

## 基本流程

- `url = 'http://www.baidu.com'` 用字符串表示url

- `headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36'}`设置头部信息

- `request = urllib.request.Request(url=url, headers=headers)`创建request的对象

- `response = urllib.request.urlopen(request)`获取网站响应的信息对象

- `content = response.read().decode('utf-8')`返回响应信息中的内容

- ```python
  with open('filepath', 'w', encoding='utf-8') as fp:
      fp.write(content)
  ```

  将内容写入到文本中	

- 如果需要登录的网站，直接在`headers`上加入cookie元素,`headers = {'cookie': '这里是网络请求头的cookie内容'}` 

- 下载图片:`urllib.request.urlretrieve('文件网络地址', '文件保存的本地地址')`

## handler处理器

- 解决动态cookie和代理的问题

基本过程：

- `handler = urllib.request.HTTPHandler()`创建handler对象
- `opener = urllib.request.build_opener(handler)`创建opener对象
- `response = opener.open(request)`调用open方法

### 代理

 更改的步骤：

- `proxies = {'http': 'IP:port'}`
- `handler = urllib.rrequest.ProxyHandler(proxies= proxies)`

### 代理池

```python
proxies_pool = [
    {'http': '192.168.1.1:100'},
    {'http': '192.168.2.1:100'}
]
import random
proxies = random.choice(proxies_pool)
```

# xpath

- 使用xpath解析html文件

- 基本使用：

```python
from lxml import etree

#解析本地文件
tree = etree.parse('file path')
#这个路劲开头必须使用//,相当于根目录下
li_list = tree.xpath('//body/li')#查找body所有孩子中的li元素，并返回一个列表。/表示儿子
li_list = tree.xpath('//body//li')#查找body所有子孙后代中li元素，返回列表。//表示子孙
li_list = tree.xpath('//body/ul/li[@id]')#找到的所有li中，有id标签的元素。
li_list = tree.xpath('//body//li[@id="id值"]')#在所有找到的li中，id等于id值的元素。
li_list = tree.xpath('//body//li[@id="l1"]/@class')#返回满足条件的li元素的类型，也可以/@任意一个属性值，表示返回这个元素对应的属性
li_list = tree.xpath('//body//li/text()')#返回这些元素的值 
print(li_list)
#解析网络文件
content = response.read().decode('utf-8')
tree = etree.HTML(content)//构建解析树
result = tree.xpath('//input[@id="su"]/@value')//直接使用xpath语句找到对应的结果
#and
可以在[]中使用and表示多个条件同时成立
```

# JsonPath

- 解析json文件
- 只能解析本地文件，不能解析服务器响应的文件

- 基本使用方式：

```python
obj = json.load(open('filepath.json', 'r', encoding='utf-8'))
result_list = jsonpath.jsonpath(obj, 'jsonpath语法')
```

- jsonpath语法(与xpath语法对比)：

  | xpath   | jsonpath           | 含义                                     |
  | ------- | ------------------ | ---------------------------------------- |
  | /       | $                  | 根目录                                   |
  | /       | .                  | 子目录                                   |
  | //      | ..                 | 子孙目录                                 |
  |         | *                  | 所有元素（可能是一个包含json元素的列表） |
  |         | [(@.length-1)]     | 找到的元素中的最后一个元素               |
  |         | [:2]               | 前两个元素                               |
  | [@标签] | [?(@.键名)]        | 有该键名的键                             |
  |         | [?(@.标签>某个值)] |                                          |

# BeautifulSoup(bs4)

- HTML解析器（跟lxml一样）

- 缺点：效率没有lxml效率高

- 优点：接口设计更加人性化，使用方便

- 基础使用：

  ```python
  from bs4 import BeautifulSoup
  
  soup = BeautifulSoup(open('html file path', 'r', encoding='utf-8'), 'lxml')
  #soup = BeautifulSoup(content, 'lxml')#这里的content可以为response.read()（urllib方法）， 也可以为response.text（requests方法）
  
  
  soup.a#根据标签名a返回对应的第一个标签
  soup.a.attrs#返回对应标签的属性，属性用字典的形式返回
  
  #BeautifulSoup的相关方法
  #find()方法，返回第一个符合条件对的标签
  soup.find('a')#返回第一个标签名为a的标签
  soup.find('a', title='title name')#返回第一个标签名为title name的a标签
  soup.find('a', class_='title name')#因为class为python的关键字，因此要将标签属性名class编程class_
  
  #find_all()方法，返回符合条件的标签的列表
  soup.find_all('a')
  soup.find_all(['a', 'span', 'div'])#返回多个标签，标签名分别为a，span，div
  soup.find_all('li', limit=2)#规定返回的标签数量不能超过limit
   
  #select()方法，返回符合条件的标签的列表
  soup.select('a')
  soup.select('.a_name')#跟css语法一样，这里是类选择器，表示属性class等于a_name的标签。
  soup.select('#id_name')#表示属性id等于id_name的标签
  soup.select('li[id]')#表示li中有id属性的标签
  soup.select('li[id="l2"]')#表示找到id等于l2的标签
  soup.select('div ul li')#空格表示后代选择器
  soup.select('div > li')#>表示子代选择器
  soup.select('a, li')#表示找到所有a和li标签
  
  obj = soup.select('a')[0]#obj表示找到的结果中的第一个标签
  #获取标签的信息
  #获取标签的内容
  obj.get_text()#返回该标签中的内容
  #获取标签的属性
  obj.name#获取标签的名称
  obj.attrs#将属性名和属性值作为一个字典返回
  #获取标签的属性的值
  obj.attrs.get('class')#obj.attrs为字典，使用字典的 get方法返回对应键名的键值
  
  ```

# selenium

- 作用：模拟一个浏览器去操纵数据，如果不用浏览器的话，有的元素无法获取

- 基本使用：

  ```python
  from selenium import webdriver
  #这时候要把对应的驱动放到当前路径下
  browser = webdriver.Chrome()
  url = ""
  browser.get(url)#打开网址
  #定位标签
  button = browser.find_element(by='方式', value='对应的值')#方式有id,name,xpath,tag name(中间有空格，表示标签名),css selector(使用的是bs4的语法),link text(查找名称符合条件的链接)
  button = browser.find_elements(by='方式', value='对应的值')#这个是查找多个元素，返回列表
  ```

- 获取属性值：`button.get_attribute('属性名')`

- 获取标签名：`button.tag_name`

- 获取标签文本： `button.text`

## selenium 交互

```python
from selenium import webdriver

browser = webdriver.Chrome()
url = 'https://www.baidu.com'
browser.get(url)

import time
time.sleep(2)
input = browser.find_element('id', 'kw')
# 向输入标签输入值
input.send_keys('周杰伦')

time.sleep(2)
button = browser.find_element('id', 'su')
button.click()
# 设置js语句，并执行，让页面滚到底部，一般设置为十万
time.sleep(2)
js_botton = 'document.documentElement.scrollTop=100000'
browser.execute_script(js_botton)
# 通过xpath找到对应的按钮
time.sleep(2)
next = browser.find_element('xpath', '//a[@class="n"]')
# 点击按钮
next.click()
# 返回上一页
time.sleep(2)
browser.back()
# 返回上一步
time.sleep(2)
browser.forward()
# 退出
time.sleep(2)
browser.quit()
```

## chrome handless

- 这是一个无界面的浏览器，运行效率更快。所以这个仅仅是为了解决selenium的运行效率问题

- 使用：

  ```python
  
  from selenium import  webdriver
  from selenium.webdriver.chrome.options import Options
  
  def share_browser():
      chrome_options = Options()
      chrome_options.add_argument('--headless')
      chrome_options.add_argument('--disable-gpu')
  
      # 前面是固定写死了的，后面chrome的可执行文件路径需要自己写。这里的字符串路径前面加r是为了让\不转译
      path = r'C:\Program Files\Google\Chrome\Application\chrome.exe'
      chrome_options.binary_location = path
      browser = webdriver.Chrome(chrome_options=chrome_options)
      return browser
  
  # 这里的browser的方法跟selenium的完全一样
  browser = share_browser()
  url = 'https://www.baidu.com'
  
  browser.get(url)
  
  browser.save_screenshot('baidu.png')#保存截图
  ```

# requests

- 比`urllib`更加简单

- 基本使用

  ```python
  import requests
  
  url = 'http://www.baidu.com'
  
  #一个类型
  response = requests.get(url)
  
  #六个属性
  response.encoding = 'utf-8'
  print(response.url)  #这是url的地址
  print(response.text) #返回网页源码的字符串
  print(response.content)# 这里的内容返回的是二进制的数据
  print(response.status_code)#返回响应的状态码
  print(response.headers)#返回响应头
  ```

- get请求：
  ```python
  import requests
  
  
  url = 'https://www.baidu.com/s?'
  
  # 请求头
  headers = {
  'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36'
  }
  
  # 参数
  data = {
      'wd': '北京'
  }
  
  # requests.get(url, params, kwargs) #param是参数。kwargs是字典，表示请求头
  response = requests.get(url=url, params=data, headers=headers)
  content = response.text
  print(content )
  ```

- post请求：

  ```python
  import requests
  
  #百度翻译的接口
  url = 'https://fanyi.baidu.com/sug'
  headers = {
  'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36'
  }
  data = {
      'kw': 'test'
  }
  
  # 使用post请求,post(url, data， kwargs)， #这里的data是传入的参数，kwargs是字典，表示请求头
  response = requests.post(url=url, headers=headers, data=data)
  content = response.text
  
  import json
  
  obj = json.loads(content)
  print(obj)
  ```

- 使用代理:

  ```python
  import requests
  
  
  url = 'http://www.baidu.com/s?'
  
  # 请求头
  headers = {
  'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36',
  }
  
  # 参数
  data = {
      'wd': 'ip'
  }
  
  proxy = {
      'http':'36.134.91.82:8888' #代理信息
  }
  # requests.get(url, params, kwargs) #param是参数
  response = requests.get(url=url, params=data, headers=headers, proxies=proxy)
  content = response.text
  print(content)
  ```

- 找到标签的隐藏属性和绕过图片验证码：

  ```python
  import requests
  
  
  url = 'https://so.gushiwen.cn/user/login.aspx?from=http://so.gushiwen.cn/user/collect.aspx?'
  
  # 请求头
  headers = {
  
  'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36',
  }
  
  # 参数
  data = {
      'wd': 'ip'
  }
  
  proxy = {
      'http':'36.134.91.82:8888'
  }
  # requests.get(url, params, kwargs) #param是参数
  response = requests.get(url=url,  headers=headers, proxies=proxy)
  content = response.text
  
  from bs4 import BeautifulSoup
  
  soup = BeautifulSoup(response.text, 'lxml')
  viewstate = soup.select('#__VIEWSTATE')[0].attrs.get('value')
  viewstate_generator = soup.select('#__VIEWSTATEGENERATOR')[0].attrs.get('value')
  code = soup.select('#imgCode')[0].attrs.get('src')
  code_url = 'https://so.gushiwen.cn/' + code
  
  
  session = requests.session()
  # 得到验证码图片的url的内容
  response_code = session.get(code_url)
  # 保存图片的二进制码
  content_code = response_code.content
  # 以二进制方式写入文件
  with open('code.jpg', 'wb' ) as fp:
      fp.write(content_code)
  code_value = input('input the code:')
  
  data = {
      '_VIEWSTATE': viewstate,
      '__VIEWSTATEGENERATOR': viewstate_generator,
      'from': 'http://so.gushiwen.cn/user/collect.aspx',
      ' email': 'shannon_hsuing@163.com',
       'pwd': '1141236071gsw',
      "code": code_value,
      'denglu': '登录'
  }
  
  url_post = 'https://so.gushiwen.cn/user/login.aspx?from=http%3a%2f%2fso.gushiwen.cn%2fuser%2fcollect.aspx'
  
  # 让着次跟上次访问图片是同一个请求
  response_post = session.post(url=url, data=data, headers=headers)
  
  content_post = response_post.text
  with open('gushiwen.html', 'w', encoding='utf-8') as fp:
      fp.write(content_post)
  ```

- 除了自己识别图片，还可以使用企业专门的验证码识别网站（比如超级鹰）

# scrapy

- scrapy是一个为了爬取网站数据，提取结构性数据而编写的应用框架。

- 基本使用：

  - ```python
    创建项目:在终端使用命令scrapy startproject 项目名称
    在spiders目录下使用命令：scrapy genspider 爬虫名称 网站地址(不要带http://)
    ```

  - 生成的爬虫文件：

    ```python
    import scrapy
    
    class BaiduSpider(scrapy.Spider):
        name = 'baidu'
        allowed_domains = ['www.baidu.com']
        # 第一次要访问的域名，start_urls是在allowed_domains前面添加http://,在其后面添加/
        start_urls = ['http://www.baidu.com/']
        # 这里的response是执行urllib.request.urlopen()或者requests.get(url)后返回的对象，因此没必要再去执行这些代码。
        def parse(self, response):
            pass
    
    ```

  - 个人爬虫没必要遵守robots协议:

    ```python
    在项目的setting.py中
    修改：ROBOTSTXT_OBEY = False
    ```

  - 最后在终端执行：`scrapy crawl 爬虫名称`

## scrapy项目的结构

项目名称

> 项目名称
>
> > spiders文件夹（存储爬虫文件)
> >
> > > init.py
> > >
> > > 自定义的爬虫文件(***核心功能文件***)
> >
> > init
> >
> > items(定义数据结构)
> >
> > middleware(中间件 比如实现代理)
> >
> > piplines(处理下载的数据)
> >
> > settings(配置文件)

## response的属性和方法

- `text`相应数据的字符串
- `body`相应数据的二进制数据
- `xpath('xpath语句')`返回selector对象的列表
- `extract()`提取selector对象的data属性值
- `extract_first()`提取selector列表的第一个元素的data属性值

## scrapy工作原理

![image-20230807135549317](C:\Users\www15\Nutstore\1\notes\爬虫\assets\image-20230807135549317.png)

## scrapy shell（一般用不到）

```python
在终端使用scrapy shell 网址
直接打开ipython（前提是安装了ipython）
然后再ipython中使用
```

## 链接提取(crawl_spider)

```python
import scrapy
from scrapy.linkextractors import LinkExtractor

class ReadbookSpider(scrapy.Spider):
    name = 'readbook'
    allowed_domains = ['www.dushu.com']
    start_urls = ['https://www.dushu.com/book/1175.html']

    def parse(self, response):
        # link = LinkExtractor(allow=r'/book/1175_\d+\.html')使用正则提取链接
        link = LinkExtractor(restrict_xpaths=r'//div[@class="pages"]/a/@href')
        print(link.extract_links(response))

```

- 创建爬虫文件`scrapy genspider -t crawl 爬虫文件名称 爬取的网页`

- 爬虫文件的配置

  ```python
  import scrapy
  from scrapy.linkextractors import LinkExtractor
  from scrapy.spiders import CrawlSpider, Rule
  
  
  class ReadSpider(CrawlSpider):
      name = 'read'
      allowed_domains = ['www.dushu.com']
      start_urls = ['https://www.dushu.com/book/1175_1.html']
  
      rules = (
          Rule(LinkExtractor(allow=r'/book/1175_\d+\.html'), callback='parse_item', follow=False),
      )
  
      def parse_item(self, response):
          img_list = response.xpath('//li//div[@class="book-info"]//img')
          for img in img_list:
              name = img.xpath('./@alt').extract_first()
              src = img.xpath('./@data-original').extract_first()
  
              from scrapy_readbook.items import ScrapyReadbookItem
              book = ScrapyReadbookItem(name=name, src=src)
              yield book
  
  
  ```

- 不想运行的时候查看信息，而是将日志信息放到一个文件中.在settings设置`LOG_FILE = 'logdemo.log'`，这样将在当前目录下常见日志文件。

## scrapy的post请求

- 项目和爬虫 文件的创建跟get请求相同，都是通过找到的接口去生成爬虫文件

- 基本使用：

  ```python
   import scrapy
  import json
  
  class TestpostSpider(scrapy.Spider):
      name = 'testpost'
      allowed_domains = ['https://fanyi.baidu.com/sug']
  
      def start_requests(self):
          url = 'https://fanyi.baidu.com/sug'
          data = {#请求数据
              'kw': 'test',
          }
          yield scrapy.FormRequest(url=url, formdata=data, callback=self.parse_second)
      def parse_second(self, response):#reponse就是这个post请求的返回值
          content = response.text
          obj = json.loads(content)
          print(obj)
  ```

  
